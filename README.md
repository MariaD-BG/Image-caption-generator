# Image Caption Generation
## Мария Дренчева, 2-ри курс, Информатика

### Цел на проекта
Дефинираме, тренираме и оценяваме свой Machine Learning модел за генериране на текст, описващ подадена снимка. Освен самия модел и скриптове за неговото трениране и оценяване върху тестовата част от датасета, в проекта има и streamlit приложение, което позволява на потребителя да прикачи снимка и да сравни генерирания текст от нашия модел и от baseline такъв. Разбира се, baseline-ът се справя значително по-добре, но все пак целта не е да решим отворен research проблем :)

### Детайли по имплементацията

## Архитектура на модела
Моделът представлява encoder-decoder модел, където за encoder (тоест генерирани на подходящи feature-и от суровите пиксели) използваме готовия модел CLIP (https://openai.com/index/clip/). Decoder-ът е LSTM модел с embedding и output layer. За dictionary от token-и използваме тези на CLIP модела. При inference използваме beam search за намиране на оптималната последователност, вместо greedy approach с директно избиране на най-вероятната следваща дума.
Тренирането се извършва върху Flickr8K (https://www.kaggle.com/datasets/adityajn105/flickr8k) -- набор от около 8000 изображения със съответстващо им описание (едно изречение за всяка).