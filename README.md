# Image Caption Generation
## Мария Дренчева, 2-ри курс, Информатика
github: https://github.com/MariaD-BG/Image-caption-generator

### Цел на проекта
Дефинираме, тренираме и оценяваме свой Machine Learning модел за генериране на текст, описващ подадена снимка. Освен самия модел и скриптове за неговото трениране и оценяване върху тестовата част от датасета, в проекта има и streamlit приложение, което позволява на потребителя да прикачи снимка и да сравни генерирания текст от нашия модел и от baseline такъв. Разбира се, baseline-ът се справя значително по-добре, но все пак целта не е да решим отворен research проблем :)

### Детайли по имплементацията
Моделът ни следва encoder-decoder архитектура. За encoder използваме модела на openai CLIP, който от изображение генерира embedding вектор. Decoder-ът е LSTM, който тренираме, като подаваме като първи token този embedding вектор. Така подаваме първо визуалната информация като своеобразен input. При генериране (след като вече сме тренирали модела) вместо greedy approach използваме beam search за намиране на оптималната последователност от думи.

## Архитектура на модела
Моделът представлява encoder-decoder модел, където за encoder (тоест генерирани на подходящи feature-и от суровите пиксели) използваме готовия модел CLIP (https://openai.com/index/clip/). Decoder-ът е LSTM модел с embedding и output layer. За dictionary от token-и използваме тези на CLIP модела. При inference използваме beam search за намиране на оптималната последователност, вместо greedy approach с директно избиране на най-вероятната следваща дума.
Тренирането се извършва върху Flickr8K (https://www.kaggle.com/datasets/adityajn105/flickr8k) -- набор от около 8000 изображения със съответстващо им описание (по 5 версии на едно изречение за всяка).

## Data Analysis
Включен е и jupyter notebook с exploratory data analysis на Flickr8K. За да го ръннете, инсталирайте pip install ipykernel към вече създадения environment, който включва всичко от requirements.txt.

## Как да работите с модела:

# *Преди да започнете:*
*Не забравяйте да следвате инструкциите в INSTALL.md и да свалите данните от Flickr8k в папка data!*

1) Сваляте от интернет необходимите за encoder-а модели: за целта рънвате python load_models_local.py. Ще се появи папка local_clip_model, в която за запазени свалени tokenizer, processor.

2) Подготвяте данните: python process.py. В папката data, в която сте запазили данните, ще се появи файл features.pt. Това са precompute-натите резултати от CLIP encoder-а върху всички изображения в нашите данни. (на CPU това може да отнеме до 15-тина минути; на GPU са 2-3)

3) Ако искате да разгледате данните за трениране, отваряте eda.ipynb, добавяте pip install ipykernel към environment-а си и рънвате отделните клетки на notebook-а.

4) Тренирате модела: python train.py. Ще се появи папка checkpoints, в която периодично ще се обновява файлът checkpoint.pth. След всяка епоха се запазва моделът с най-добър validation loss. Ще се появи и папка plots, в която файлът train_loss.png съдържа графика на train и validation loss-ът. Той също се обновява периодично: на всеки 20 епохи, така че можете да го следите в реално време. На a6000 gpu 50 епохи отнемат около половин час.

5) Тествате модела: python eval.py --checkpoint checkpoints/checkpoint.pth . Изчисляването на test score-а е бързо, но BLEU score-ът е по-бавен (може да отнеме до 20-тина минути в зависимост на какво рънвате).

6) Ако искате да тествате само нашия модел само на едно изображение, може да го направите с python inference.py  --checkpoint checkpoints/checkpoint.pth --image <path-to-jpg>

7) За да сравните нашия модел с baseline такъв, можете да ръннете streamlit приложението със streamlit run app.py. Там изберете дали ще работите с нашия модел или с baseline, качете снимка и вижте резултата.